# =============================================================================
# File: Backend/module/document_processor/vector_db_manager.py
# =============================================================================
# SeShat - Vector DB Manager (ì œí’ˆë³„ FAISS ì¸ë±ìŠ¤ ìë™ ìƒì„±)
# -----------------------------------------------------------------------------
# ëª©ì :
#   - document_pr.pyì—ì„œ ìƒì„±ëœ ì œí’ˆë³„ ì²­í¬ íŒŒì¼ì„ ì½ì–´
#     ê° ì œí’ˆë³„ë¡œ ë…ë¦½ì ì¸ FAISS ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìƒì„±
#   - í† í° ì´ˆê³¼ ë°©ì§€ë¥¼ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬(batch embedding)
#   - í•œê¸€/ê³µë°±/íŠ¹ìˆ˜ë¬¸ì í´ë”ëª… ìë™ ì •ê·œí™” (Windows ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€)
#   - ì§„í–‰ë¥  í‘œì‹œ ë° ë¡œê·¸ ì¶œë ¥ í¬í•¨
#
# ì…ë ¥:
#   - Backend/data/chunk_store/{product_id}_chunks.json
#
# ì¶œë ¥:
#   - Backend/data/faiss_index/{product_id}/index.faiss
#   - Backend/data/faiss_index/{product_id}/docstore.pkl
#
# ì‹¤í–‰:
#   (venv) $ python -m Backend.module.document_processor.vector_db_manager
#
# ìš”êµ¬ íŒ¨í‚¤ì§€:
#   pip install langchain langchain-community langchain-openai faiss-cpu tqdm
#   pip install openai
#
# í™˜ê²½ ì„¤ì •:
#   .env íŒŒì¼ì— OPENAI_API_KEY ì„¤ì • í•„ìš”
# =============================================================================

from __future__ import annotations
import json
import re
import hashlib
from pathlib import Path
from tqdm import tqdm
from Backend.core.config import get_settings

# LangChain imports
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document


# ------------------------------
# 1) íŒŒì¼ëª… ì•ˆì „í™” í•¨ìˆ˜ (í•œê¸€ í¬í•¨ ì™„ì „ ëŒ€ì‘)
# ------------------------------
def sanitize_filename(name: str) -> str:
    """
    í•œê¸€, ê³µë°±, íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ì´ë¦„ì„ ASCII ì•ˆì „ ë¬¸ìì—´ë¡œ ë³€í™˜.
    ì˜ˆ: "2024ë…„_SIF-ì œí’ˆë§¤ë‰´ì–¼" â†’ "HAN_2024_SIF_5a1b2c"
    """
    try:
        # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸
        if any(ord(ch) > 127 for ch in name):
            hashed = hashlib.md5(name.encode("utf-8")).hexdigest()[:6]
            # í•œê¸€/íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ASCIIë§Œ ë‚¨ê¹€
            ascii_name = re.sub(r"[ã„±-ã…ã…-ã…£ê°€-í£]+", "", name)
            ascii_name = re.sub(r"[^\w\-]+", "_", ascii_name)
            return f"HAN_{ascii_name[:40]}_{hashed}"
        # ì˜ì–´/ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°
        return re.sub(r"[^\w\-_.]", "_", name)[:80]
    except Exception:
        return "unknown_doc"


# ------------------------------
# 2) ì²­í¬ ë¡œë“œ í•¨ìˆ˜
# ------------------------------
def load_chunks(json_path: Path):
    """ì œí’ˆë³„ ì²­í¬ JSONì„ ë¡œë“œí•˜ì—¬ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    try:
        data = json.load(open(json_path, "r", encoding="utf-8"))
        docs = []
        for item in data:
            text = item.get("text", "").strip()
            if not text:
                continue
            metadata = {"document_id": item.get("document_id")}
            docs.append(Document(page_content=text, metadata=metadata))
        return docs
    except Exception as e:
        print(f"âš ï¸ {json_path.name} ë¡œë“œ ì‹¤íŒ¨ ({e})")
        return []


# ------------------------------
# 3) FAISS ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜
# ------------------------------
def build_faiss_index(product_id: str, docs: list[Document], out_dir: Path, batch_size: int = 80):
    """ì£¼ì–´ì§„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥"""
    if not docs:
        print(f"âš ï¸ {product_id}: ì²­í¬ ì—†ìŒ â†’ ì¸ë±ìŠ¤ ê±´ë„ˆëœ€")
        return

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # âœ… í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ëŒ€ì‘ í´ë”ëª… ë³€í™˜
    safe_id = sanitize_filename(product_id)
    product_dir = out_dir / safe_id
    product_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n[BUILD] {product_id} â†’ {product_dir}")

    all_docs = []
    vectorstore = None

    # ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”© (í† í° ì´ˆê³¼ ë°©ì§€)
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        print(f" â†’ Embedding batch {i//batch_size + 1}/{(len(docs) // batch_size) + 1} ({len(batch)}ê°œ ì²­í¬)")
        try:
            if i == 0:
                vectorstore = FAISS.from_documents(batch, embeddings)
            else:
                batch_store = FAISS.from_documents(batch, embeddings)
                vectorstore.merge_from(batch_store)
        except Exception as e:
            print(f"âš ï¸ {product_id}: ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({e})")
            continue
        all_docs.extend(batch)

    if vectorstore:
        try:
            vectorstore.save_local(str(product_dir))
            print(f"âœ… {product_id} ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ({len(all_docs)}ê°œ ì²­í¬)")
        except Exception as e:
            print(f"âŒ {product_id}: ì €ì¥ ì‹¤íŒ¨ ({e})")
    else:
        print(f"âŒ {product_id}: ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨ (ë¹„ì–´ ìˆìŒ)")


# ------------------------------
# 4) ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ------------------------------
def run_vector_indexing():
    """Backend/data/chunk_storeì˜ ëª¨ë“  ì œí’ˆë³„ ì²­í¬ íŒŒì¼ì„ ìˆœíšŒí•˜ë©° ì¸ë±ì‹±"""
    _, paths = get_settings()
    chunk_dir = Path(paths.DATA_DIR) / "chunk_store"
    out_dir = Path(paths.DATA_DIR) / "faiss_index"
    out_dir.mkdir(parents=True, exist_ok=True)

    print(f"[INIT] Building FAISS indices per product in {chunk_dir}...\n")

    chunk_files = sorted(chunk_dir.glob("*_chunks.json"))
    if not chunk_files:
        print("âŒ ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € document_pr.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    for file in tqdm(chunk_files, desc="Building Product Indices"):
        product_id = file.stem.replace("_chunks", "")
        docs = load_chunks(file)
        build_faiss_index(product_id, docs, out_dir)

    print("\nğŸ¯ ëª¨ë“  ì œí’ˆë³„ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")


# ------------------------------
# 5) CLI ì‹¤í–‰
# ------------------------------
if __name__ == "__main__":
    run_vector_indexing()
