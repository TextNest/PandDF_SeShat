# =============================================================================
# File: Backend/module/qa_service_chat.py
# =============================================================================
# SeShat - ì œí’ˆë³„ ì§ˆì˜ì‘ë‹µ ì½˜ì†” ì¸í„°í˜ì´ìŠ¤ (RAG ê¸°ë°˜)
# -----------------------------------------------------------------------------
# ëª©ì :
#   - ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì œí’ˆëª…(model_name)ì— ë”°ë¼ í•´ë‹¹ ì œí’ˆì˜ ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìë™ ë¡œë“œ
#   - ëª¨ë¸ëª… ì¸ë±ìŠ¤ê°€ ì—†ì„ ê²½ìš° í†µí•© ì¸ë±ìŠ¤(index.faiss)ë¡œ í´ë°±
#   - LangChain Retriever + LLM(QA Chain) ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰
#
# ì‹¤í–‰:
#   (venv) $ python -m Backend.module.qa_service_chat
#
# ìš”êµ¬ íŒ¨í‚¤ì§€:
#   pip install langchain langchain-community langchain-openai faiss-cpu tqdm
#   pip install openai
#
# í™˜ê²½:
#   .env íŒŒì¼ì— OPENAI_API_KEY ì„¤ì • í•„ìš”
# =============================================================================

from __future__ import annotations
import os
from pathlib import Path
from Backend.core.config import get_settings
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ------------------------------
# 1) RAG ì²´ì¸ ë¡œë”
# ------------------------------
def load_product_rag_chain(product_id: str):
    """
    ì œí’ˆëª… ê¸°ë°˜ RAG ì²´ì¸ ìƒì„±
    1ï¸âƒ£ í•´ë‹¹ ì œí’ˆì˜ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    2ï¸âƒ£ ì—†ìœ¼ë©´ í†µí•© ì¸ë±ìŠ¤(index.faiss)ë¡œ í´ë°±
    """
    _, paths = get_settings()
    base_dir = Path(paths.DATA_DIR) / "faiss_index"
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    product_dir = base_dir / product_id
    fallback_index = base_dir / "documents"

    if product_dir.exists():
        index_path = product_dir / "index.faiss"
        store_dir = product_dir
        print(f"[INIT] Loading FAISS index from: {product_dir}")
    elif fallback_index.exists():
        store_dir = fallback_index
        print(f"âš ï¸ [{product_id}] ì „ìš© ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í†µí•© ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print(f"[INIT] Loading FAISS index from: {fallback_index}")
    else:
        raise FileNotFoundError("âŒ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # FAISS ë¡œë“œ
    vectorstore = FAISS.load_local(
        str(store_dir),
        embeddings,
        allow_dangerous_deserialization=True
    )

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_template = """
    ë‹¹ì‹ ì€ {product_id} ì œí’ˆì˜ ê³µì‹ ì„¤ëª…ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì…ë‹ˆë‹¤:
    ------------------------
    {context}
    ------------------------
    ì§ˆë¬¸: {question}
    ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ, ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    """
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question", "product_id"]
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt.partial(product_id=product_id)}
    )

    return chain


# ------------------------------
# 2) ì½˜ì†” ì¸í„°ë™í‹°ë¸Œ ì±—
# ------------------------------
def interactive_chat():
    print("\nğŸ“˜ SeShat - ì œí’ˆë³„ AI ë§¤ë‰´ì–¼ ì–´ì‹œìŠ¤í„´íŠ¸")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    product_id = input("ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: SVC-ST25WIF_USER): ").strip()

    try:
        chain = load_product_rag_chain(product_id)
    except FileNotFoundError as e:
        print(f"âŒ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e})")
        return

    print(f"\nâœ… [{product_id}] ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ. ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥.\n")

    while True:
        question = input("â“ ì§ˆë¬¸: ").strip()
        if question.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤. ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”!")
            break

        try:
            result = chain({"query": question})
            answer = result.get("result", "").strip()
            sources = result.get("source_documents", [])

            print(f"\nğŸ’¬ AI: {answer}\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

            # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
            if sources:
                print("ğŸ“š [REFERENCE DOCS]")
                for i, doc in enumerate(sources[:3], 1):
                    name = doc.metadata.get("document_id", "unknown")
                    print(f"- ({i}) {name} | {len(doc.page_content)}ì")
                print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ------------------------------
# 3) CLI ì‹¤í–‰
# ------------------------------
if __name__ == "__main__":
    interactive_chat()
